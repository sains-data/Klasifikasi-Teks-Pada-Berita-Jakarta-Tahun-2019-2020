# -*- coding: utf-8 -*-
"""Klasifikasi Teks Pada Berita Jakarta Tahun 2019â€“2020.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NjGUtjVCKEytx-iwsLZsJKu76yuv5woj
"""

import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt

df = pd.read_csv('/content/combined_data.csv')
df.head(10)

df.tail(10)

kategori = pd.get_dummies(df.tonality)
df = pd.concat([df, kategori], axis=1)
df = df.drop(columns='tonality')
df.sample(5)

feature = df['isu']
label = df[['NEGATIF', 'NETRAL', 'POSITIF']].values

from sklearn.model_selection import train_test_split
feature_latih, feature_test, label_latih, label_test = train_test_split(feature, label, test_size=0.2, random_state=42)

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences


pad_type = 'pre'
trunc_type = 'pre'

# Tokenize our training data
tokenizer = Tokenizer(num_words=2000, oov_token='x')
tokenizer.fit_on_texts(feature_latih)
tokenizer.fit_on_texts(feature_test)

# Encode training data sentences into sequences
sekuens_latih = tokenizer.texts_to_sequences(feature_latih)
sekuens_test = tokenizer.texts_to_sequences(feature_test)

# Get max training sequence length
maxlen = max([len(x) for x in sekuens_latih])

# Pad the training sequences
padded_latih = pad_sequences(sekuens_latih, padding=pad_type, truncating=trunc_type, maxlen=maxlen)
padded_test = pad_sequences(sekuens_test, padding=pad_type, truncating=trunc_type, maxlen=maxlen)

model = tf.keras.Sequential([
  tf.keras.layers.Embedding(input_dim=2000, output_dim=8),
  tf.keras.layers.Dropout(0.5),
  tf.keras.layers.LSTM(32),
  tf.keras.layers.Dropout(0.5),
  tf.keras.layers.Dense(32, activation='relu'),
  tf.keras.layers.Dropout(0.5),
  tf.keras.layers.Dense(3, activation='softmax')
  ])
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

model.summary()

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.9 and logs.get('val_accuracy')>0.9):
      print("\nAkurasi telah mencapai >90%!")
      self.model.stop_training = True
callbacks = myCallback()

num_epochs = 40
hist = model.fit(padded_latih, label_latih, epochs=num_epochs,
                    validation_data=(padded_test, label_test),
                    verbose=2, callbacks=[callbacks])

# summarize history for accuracy
plt.plot(hist.history['accuracy'])
plt.plot(hist.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# summarize history for loss
plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()

# function to make predictions
def make_predictions(model, tokenizer, ml):
  classes = ['Negatif', 'Netral', 'Positif']
  while True:
    text = input('Masukkan isu berita: ')
    text = [text]
    t = tokenizer.texts_to_sequences(text)
    t = pad_sequences(t, padding='pre', truncating='pre', maxlen=ml)
    pred_t = model.predict(t)
    print('Prediksi: ', classes[pred_t.argmax()])
    lagi = input('Apakah ingin mencoba lagi? (y/n) :')
    if lagi=='n' or lagi=='N':
      break

make_predictions(model, tokenizer, maxlen)